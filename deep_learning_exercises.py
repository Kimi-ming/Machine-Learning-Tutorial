# -*- coding: utf-8 -*-
# æ·±åº¦å­¦ä¹ å®è·µç»ƒä¹ 
# Deep Learning Exercises: åŠ¨æ‰‹å®è·µæ·±åº¦å­¦ä¹ é¡¹ç›®

import random
import math
import json

def exercises_introduction():
    """æ·±åº¦å­¦ä¹ ç»ƒä¹ ä»‹ç»"""
    print("=== æ·±åº¦å­¦ä¹ å®è·µç»ƒä¹  ===")
    print("é€šè¿‡åŠ¨æ‰‹å®è·µå·©å›ºæ·±åº¦å­¦ä¹ çŸ¥è¯†")
    print()
    print("ç»ƒä¹ ç‰¹ç‚¹:")
    print("â€¢ å¾ªåºæ¸è¿›ï¼šä»åŸºç¡€åˆ°é«˜çº§")
    print("â€¢ å®è·µå¯¼å‘ï¼šè§£å†³å®é™…é—®é¢˜")
    print("â€¢ ä»£ç å®ç°ï¼šäº²æ‰‹ç¼–å†™ç®—æ³•")
    print("â€¢ æ·±å…¥ç†è§£ï¼šæŒæ¡åº•å±‚åŸç†")
    print()

class DeepLearningProject:
    """æ·±åº¦å­¦ä¹ é¡¹ç›®åŸºç±»"""
    
    def __init__(self, name, difficulty, description):
        self.name = name
        self.difficulty = difficulty  # 1-5: ç®€å•åˆ°å›°éš¾
        self.description = description
        self.completed = False
    
    def get_info(self):
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        difficulty_stars = "â˜…" * self.difficulty + "â˜†" * (5 - self.difficulty)
        status = "âœ… å·²å®Œæˆ" if self.completed else "â³ å¾…å®Œæˆ"
        
        return f"""
ã€{self.name}ã€‘
éš¾åº¦: {difficulty_stars} ({self.difficulty}/5)
çŠ¶æ€: {status}
æè¿°: {self.description}
"""

def exercise_1_perceptron():
    """ç»ƒä¹ 1ï¼šå®ç°æ„ŸçŸ¥æœº"""
    print("\n" + "="*50)
    print("ç»ƒä¹  1: å®ç°å•å±‚æ„ŸçŸ¥æœº")
    print("="*50)
    
    class Perceptron:
        """å•å±‚æ„ŸçŸ¥æœºå®ç°"""
        
        def __init__(self, input_size, learning_rate=0.1):
            # éšæœºåˆå§‹åŒ–æƒé‡å’Œåç½®
            self.weights = [random.uniform(-1, 1) for _ in range(input_size)]
            self.bias = random.uniform(-1, 1)
            self.learning_rate = learning_rate
            self.training_errors = []
            
        def activation(self, x):
            """é˜¶è·ƒæ¿€æ´»å‡½æ•°"""
            return 1 if x >= 0 else 0
            
        def predict(self, inputs):
            """é¢„æµ‹"""
            weighted_sum = sum(w * x for w, x in zip(self.weights, inputs))
            weighted_sum += self.bias
            return self.activation(weighted_sum)
            
        def train(self, training_data, epochs=100):
            """è®­ç»ƒæ„ŸçŸ¥æœº"""
            for epoch in range(epochs):
                errors = 0
                for inputs, target in training_data:
                    prediction = self.predict(inputs)
                    error = target - prediction
                    
                    # æ›´æ–°æƒé‡å’Œåç½®
                    if error != 0:
                        errors += 1
                        for i in range(len(self.weights)):
                            self.weights[i] += self.learning_rate * error * inputs[i]
                        self.bias += self.learning_rate * error
                
                self.training_errors.append(errors)
                if errors == 0:
                    print(f"è®­ç»ƒåœ¨ç¬¬ {epoch + 1} è½®æ”¶æ•›ï¼")
                    break
    
    # ä»»åŠ¡ï¼šå­¦ä¹ ANDé€»è¾‘é—¨
    print("\nä»»åŠ¡ï¼šå­¦ä¹ ANDé€»è¾‘é—¨")
    print("ANDé€»è¾‘çœŸå€¼è¡¨ï¼š")
    print("è¾“å…¥1 | è¾“å…¥2 | è¾“å‡º")
    print("-----|-------|-----")
    print("  0  |   0   |  0")
    print("  1  |   0   |  0") 
    print("  0  |   1   |  0")
    print("  1  |   1   |  1")
    
    # è®­ç»ƒæ•°æ®
    and_data = [
        ([0, 0], 0),
        ([1, 0], 0),
        ([0, 1], 0),
        ([1, 1], 1)
    ]
    
    # åˆ›å»ºå’Œè®­ç»ƒæ„ŸçŸ¥æœº
    perceptron = Perceptron(input_size=2)
    print(f"\nåˆå§‹æƒé‡: [{perceptron.weights[0]:.3f}, {perceptron.weights[1]:.3f}]")
    print(f"åˆå§‹åç½®: {perceptron.bias:.3f}")
    
    perceptron.train(and_data, epochs=100)
    
    print(f"\næœ€ç»ˆæƒé‡: [{perceptron.weights[0]:.3f}, {perceptron.weights[1]:.3f}]")
    print(f"æœ€ç»ˆåç½®: {perceptron.bias:.3f}")
    
    # æµ‹è¯•ç»“æœ
    print(f"\næµ‹è¯•ç»“æœï¼š")
    print("è¾“å…¥ | é¢„æµ‹ | å®é™…")
    print("----|------|----")
    for inputs, target in and_data:
        prediction = perceptron.predict(inputs)
        print(f"{inputs} |  {prediction}   |  {target}")
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ æ„ŸçŸ¥æœºèƒ½å¤Ÿå­¦ä¹ çº¿æ€§å¯åˆ†çš„é—®é¢˜")
    print("â€¢ æƒé‡æ›´æ–°è§„åˆ™ï¼šw = w + Î±(t-y)x")
    print("â€¢ ANDé—¨æ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œæ‰€ä»¥èƒ½å¤Ÿæ”¶æ•›")
    print("â€¢ å°è¯•XORé—¨ä¼šå‘ç°æ— æ³•æ”¶æ•›ï¼ˆéœ€è¦å¤šå±‚ç½‘ç»œï¼‰")

def exercise_2_mlp_xor():
    """ç»ƒä¹ 2ï¼šå¤šå±‚æ„ŸçŸ¥æœºè§£å†³XORé—®é¢˜"""
    print("\n" + "="*50)
    print("ç»ƒä¹  2: å¤šå±‚æ„ŸçŸ¥æœºè§£å†³XORé—®é¢˜")
    print("="*50)
    
    class MLP:
        """å¤šå±‚æ„ŸçŸ¥æœº"""
        
        def __init__(self):
            # ç½‘ç»œç»“æ„: 2è¾“å…¥ -> 2éšè— -> 1è¾“å‡º
            # éšè—å±‚æƒé‡
            self.W1 = [[random.uniform(-2, 2) for _ in range(2)] for _ in range(2)]
            self.b1 = [random.uniform(-1, 1) for _ in range(2)]
            
            # è¾“å‡ºå±‚æƒé‡  
            self.W2 = [random.uniform(-2, 2) for _ in range(2)]
            self.b2 = random.uniform(-1, 1)
            
            self.learning_rate = 1.0
            
        def sigmoid(self, x):
            """Sigmoidæ¿€æ´»å‡½æ•°"""
            return 1.0 / (1.0 + math.exp(-max(-500, min(500, x))))
            
        def sigmoid_derivative(self, x):
            """Sigmoidå¯¼æ•°"""
            s = self.sigmoid(x)
            return s * (1 - s)
            
        def forward(self, inputs):
            """å‰å‘ä¼ æ’­"""
            # éšè—å±‚
            z1 = []
            for i in range(2):
                z = sum(self.W1[i][j] * inputs[j] for j in range(2)) + self.b1[i]
                z1.append(z)
            a1 = [self.sigmoid(z) for z in z1]
            
            # è¾“å‡ºå±‚
            z2 = sum(self.W2[i] * a1[i] for i in range(2)) + self.b2
            a2 = self.sigmoid(z2)
            
            return z1, a1, z2, a2
            
        def backward(self, inputs, target):
            """åå‘ä¼ æ’­"""
            # å‰å‘ä¼ æ’­
            z1, a1, z2, a2 = self.forward(inputs)
            
            # è¾“å‡ºå±‚è¯¯å·®
            output_error = (a2 - target) * self.sigmoid_derivative(z2)
            
            # éšè—å±‚è¯¯å·®
            hidden_errors = []
            for i in range(2):
                error = output_error * self.W2[i] * self.sigmoid_derivative(z1[i])
                hidden_errors.append(error)
            
            # æ›´æ–°è¾“å‡ºå±‚æƒé‡
            for i in range(2):
                self.W2[i] -= self.learning_rate * output_error * a1[i]
            self.b2 -= self.learning_rate * output_error
            
            # æ›´æ–°éšè—å±‚æƒé‡
            for i in range(2):
                for j in range(2):
                    self.W1[i][j] -= self.learning_rate * hidden_errors[i] * inputs[j]
                self.b1[i] -= self.learning_rate * hidden_errors[i]
            
            return a2
            
        def train(self, training_data, epochs=5000):
            """è®­ç»ƒç½‘ç»œ"""
            for epoch in range(epochs):
                total_error = 0
                for inputs, target in training_data:
                    output = self.backward(inputs, target)
                    total_error += 0.5 * (output - target) ** 2
                    
                if epoch % 1000 == 0:
                    print(f"è½®æ¬¡ {epoch:4d}: é”™è¯¯ = {total_error:.6f}")
    
    print("\nä»»åŠ¡ï¼šå­¦ä¹ XORé€»è¾‘é—¨")
    print("XORé€»è¾‘çœŸå€¼è¡¨ï¼š")
    print("è¾“å…¥1 | è¾“å…¥2 | è¾“å‡º")
    print("-----|-------|-----")
    print("  0  |   0   |  0")
    print("  1  |   0   |  1")
    print("  0  |   1   |  1") 
    print("  1  |   1   |  0")
    
    # XORè®­ç»ƒæ•°æ®
    xor_data = [
        ([0, 0], 0),
        ([1, 0], 1),
        ([0, 1], 1),
        ([1, 1], 0)
    ]
    
    # è®­ç»ƒç½‘ç»œ
    mlp = MLP()
    print(f"\nå¼€å§‹è®­ç»ƒå¤šå±‚æ„ŸçŸ¥æœº...")
    mlp.train(xor_data, epochs=5000)
    
    # æµ‹è¯•ç»“æœ
    print(f"\næµ‹è¯•ç»“æœï¼š")
    print("è¾“å…¥    | è¾“å‡º   | ç›®æ ‡")
    print("--------|-------|-----")
    for inputs, target in xor_data:
        _, _, _, output = mlp.forward(inputs)
        print(f"[{inputs[0]}, {inputs[1]}] | {output:.3f} | {target}")
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ XORé—®é¢˜æ˜¯éçº¿æ€§çš„ï¼Œéœ€è¦éšè—å±‚")
    print("â€¢ å¤šå±‚æ„ŸçŸ¥æœºé€šè¿‡åå‘ä¼ æ’­å­¦ä¹ ")
    print("â€¢ éšè—å±‚ä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¤æ‚æ¨¡å¼")
    print("â€¢ Sigmoidæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§")

def exercise_3_mini_cnn():
    """ç»ƒä¹ 3ï¼šè¿·ä½ CNNå®ç°"""
    print("\n" + "="*50)
    print("ç»ƒä¹  3: è¿·ä½ å·ç§¯ç¥ç»ç½‘ç»œ")
    print("="*50)
    
    class MiniCNN:
        """è¿·ä½ CNNç”¨äºç†è§£å·ç§¯æ¦‚å¿µ"""
        
        def __init__(self):
            # ç®€å•çš„3x3å·ç§¯æ ¸
            self.conv_filter = [
                [-1, -1, -1],
                [-1,  8, -1],
                [-1, -1, -1]
            ]
            
        def convolution(self, image, kernel, stride=1):
            """2Då·ç§¯æ“ä½œ"""
            img_h, img_w = len(image), len(image[0])
            ker_h, ker_w = len(kernel), len(kernel[0])
            
            out_h = (img_h - ker_h) // stride + 1
            out_w = (img_w - ker_w) // stride + 1
            
            output = []
            for i in range(out_h):
                row = []
                for j in range(out_w):
                    conv_sum = 0
                    for ki in range(ker_h):
                        for kj in range(ker_w):
                            img_i = i * stride + ki
                            img_j = j * stride + kj
                            conv_sum += image[img_i][img_j] * kernel[ki][kj]
                    row.append(conv_sum)
                output.append(row)
            
            return output
            
        def relu(self, feature_map):
            """ReLUæ¿€æ´»"""
            return [[max(0, val) for val in row] for row in feature_map]
            
        def max_pooling(self, feature_map, pool_size=2):
            """æœ€å¤§æ± åŒ–"""
            img_h, img_w = len(feature_map), len(feature_map[0])
            out_h = img_h // pool_size
            out_w = img_w // pool_size
            
            output = []
            for i in range(out_h):
                row = []
                for j in range(out_w):
                    max_val = float('-inf')
                    for pi in range(pool_size):
                        for pj in range(pool_size):
                            img_i = i * pool_size + pi
                            img_j = j * pool_size + pj
                            if img_i < img_h and img_j < img_w:
                                max_val = max(max_val, feature_map[img_i][img_j])
                    row.append(max_val)
                output.append(row)
                
            return output
    
    print("\nä»»åŠ¡ï¼šè¾¹ç¼˜æ£€æµ‹å·ç§¯")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆåŒ…å«è¾¹ç¼˜ï¼‰
    test_image = [
        [0, 0, 1, 1, 1],
        [0, 0, 1, 1, 1],
        [0, 0, 1, 1, 1],
        [0, 0, 1, 1, 1],
        [0, 0, 1, 1, 1]
    ]
    
    print("è¾“å…¥å›¾åƒ (5x5):")
    for row in test_image:
        print("  " + " ".join(f"{val}" for val in row))
    
    # åˆ›å»ºCNNå¹¶å¤„ç†
    cnn = MiniCNN()
    
    print(f"\nè¾¹ç¼˜æ£€æµ‹å·ç§¯æ ¸:")
    for row in cnn.conv_filter:
        print("  " + " ".join(f"{val:2d}" for val in row))
    
    # å·ç§¯æ“ä½œ
    conv_output = cnn.convolution(test_image, cnn.conv_filter)
    print(f"\nå·ç§¯è¾“å‡º ({len(conv_output)}x{len(conv_output[0])}):")
    for row in conv_output:
        print("  " + " ".join(f"{val:4.0f}" for val in row))
    
    # ReLUæ¿€æ´»
    relu_output = cnn.relu(conv_output)
    print(f"\nReLUå:")
    for row in relu_output:
        print("  " + " ".join(f"{val:4.0f}" for val in row))
    
    # æœ€å¤§æ± åŒ–
    pooled_output = cnn.max_pooling(relu_output)
    print(f"\næœ€å¤§æ± åŒ– ({len(pooled_output)}x{len(pooled_output[0])}):")
    for row in pooled_output:
        print("  " + " ".join(f"{val:4.0f}" for val in row))
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ å·ç§¯æ“ä½œæå–å±€éƒ¨ç‰¹å¾")
    print("â€¢ è¾¹ç¼˜æ£€æµ‹æ ¸çªå‡ºè¾¹ç¼˜ä¿¡æ¯")
    print("â€¢ ReLUå»é™¤è´Ÿå€¼ï¼Œå¢åŠ éçº¿æ€§")
    print("â€¢ æ± åŒ–é™ä½ç»´åº¦ï¼Œä¿ç•™é‡è¦ç‰¹å¾")

def exercise_4_simple_rnn():
    """ç»ƒä¹ 4ï¼šç®€å•RNNå®ç°"""
    print("\n" + "="*50)
    print("ç»ƒä¹  4: ç®€å•å¾ªç¯ç¥ç»ç½‘ç»œ")
    print("="*50)
    
    class SimpleRNN:
        """ç®€å•RNNå®ç°"""
        
        def __init__(self, input_size, hidden_size):
            self.input_size = input_size
            self.hidden_size = hidden_size
            
            # æƒé‡çŸ©é˜µ
            self.Wxh = [[random.uniform(-0.5, 0.5) for _ in range(hidden_size)] 
                       for _ in range(input_size)]
            self.Whh = [[random.uniform(-0.5, 0.5) for _ in range(hidden_size)] 
                       for _ in range(hidden_size)]
            self.bh = [0.0 for _ in range(hidden_size)]
            
        def tanh(self, x):
            """tanhæ¿€æ´»å‡½æ•°"""
            return math.tanh(x)
            
        def matrix_vector_mult(self, matrix, vector):
            """çŸ©é˜µå‘é‡ä¹˜æ³•"""
            result = []
            for row in matrix:
                dot = sum(m * v for m, v in zip(row, vector))
                result.append(dot)
            return result
            
        def forward(self, inputs):
            """å‰å‘ä¼ æ’­"""
            hidden = [0.0] * self.hidden_size
            outputs = []
            
            for input_vec in inputs:
                # h_t = tanh(Wxh * x_t + Whh * h_{t-1} + bh)
                h_input = self.matrix_vector_mult(self.Wxh, input_vec)
                h_hidden = self.matrix_vector_mult(self.Whh, hidden)
                
                new_hidden = []
                for i in range(self.hidden_size):
                    h_val = h_input[i] + h_hidden[i] + self.bh[i]
                    new_hidden.append(self.tanh(h_val))
                
                hidden = new_hidden
                outputs.append(hidden[:])  # å¤åˆ¶å½“å‰éšè—çŠ¶æ€
                
            return outputs
    
    print("\nä»»åŠ¡ï¼šåºåˆ—è®°å¿†æµ‹è¯•")
    print("è¾“å…¥åºåˆ—ï¼š[1, 0, 1] -> æœŸæœ›RNNèƒ½è®°ä½åºåˆ—æ¨¡å¼")
    
    # åˆ›å»ºRNN
    rnn = SimpleRNN(input_size=1, hidden_size=3)
    
    # è¾“å…¥åºåˆ—ï¼ˆæ¯ä¸ªå…ƒç´ è½¬ä¸ºå‘é‡ï¼‰
    input_sequence = [[1], [0], [1]]
    
    print(f"è¾“å…¥åºåˆ—ï¼š")
    for i, inp in enumerate(input_sequence):
        print(f"  æ—¶åˆ»{i+1}: {inp}")
    
    # å‰å‘ä¼ æ’­
    hidden_states = rnn.forward(input_sequence)
    
    print(f"\néšè—çŠ¶æ€æ¼”åŒ–ï¼š")
    for i, hidden in enumerate(hidden_states):
        print(f"  æ—¶åˆ»{i+1}: [{', '.join(f'{h:.3f}' for h in hidden)}]")
    
    print(f"\nåˆ†æï¼š")
    print("â€¢ æ¯ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€éƒ½åŒ…å«äº†å†å²ä¿¡æ¯")
    print("â€¢ éšè—çŠ¶æ€éšç€è¾“å…¥åºåˆ—ä¸æ–­æ›´æ–°")
    print("â€¢ RNNé€šè¿‡éšè—çŠ¶æ€ä¼ é€’åºåˆ—ä¿¡æ¯")
    
    # æµ‹è¯•åºåˆ—å˜åŒ–çš„å½±å“
    print(f"\næµ‹è¯•ä¸åŒåºåˆ—ï¼š")
    test_sequences = [
        [[1], [1], [1]],
        [[0], [0], [0]], 
        [[1], [0], [0]]
    ]
    
    for seq in test_sequences:
        final_hidden = rnn.forward(seq)[-1]
        seq_str = ''.join(str(x[0]) for x in seq)
        print(f"åºåˆ— [{seq_str}] -> æœ€ç»ˆçŠ¶æ€: [{', '.join(f'{h:.3f}' for h in final_hidden)}]")
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ RNNèƒ½å¤Ÿå¤„ç†å˜é•¿åºåˆ—")
    print("â€¢ éšè—çŠ¶æ€æ‰¿è½½åºåˆ—è®°å¿†")
    print("â€¢ ä¸åŒåºåˆ—äº§ç”Ÿä¸åŒçš„æœ€ç»ˆçŠ¶æ€")
    print("â€¢ ä¸ºå¤„ç†è¯­è¨€ã€æ—¶é—´åºåˆ—ç­‰æä¾›åŸºç¡€")

def exercise_5_gradient_descent():
    """ç»ƒä¹ 5ï¼šæ¢¯åº¦ä¸‹é™å¯è§†åŒ–"""
    print("\n" + "="*50)
    print("ç»ƒä¹  5: æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å¯è§†åŒ–")
    print("="*50)
    
    def quadratic_function(x):
        """äºŒæ¬¡å‡½æ•° f(x) = x^2 - 4x + 3"""
        return x**2 - 4*x + 3
        
    def quadratic_derivative(x):
        """äºŒæ¬¡å‡½æ•°å¯¼æ•° f'(x) = 2x - 4"""
        return 2*x - 4
    
    class GradientDescent:
        """æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨"""
        
        def __init__(self, learning_rate=0.1):
            self.learning_rate = learning_rate
            self.history = []
            
        def optimize(self, initial_x, max_iterations=50):
            """æ‰§è¡Œæ¢¯åº¦ä¸‹é™"""
            x = initial_x
            
            for i in range(max_iterations):
                # è®¡ç®—å‡½æ•°å€¼å’Œæ¢¯åº¦
                fx = quadratic_function(x)
                grad = quadratic_derivative(x)
                
                # è®°å½•å†å²
                self.history.append((x, fx, grad))
                
                # æ£€æŸ¥æ”¶æ•›
                if abs(grad) < 1e-6:
                    print(f"åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£æ—¶æ”¶æ•›ï¼")
                    break
                
                # æ›´æ–°å‚æ•°
                x = x - self.learning_rate * grad
                
            return x
    
    print("\nä»»åŠ¡ï¼šæ‰¾åˆ°å‡½æ•° f(x) = xÂ² - 4x + 3 çš„æœ€å°å€¼")
    print("ç†è®ºæœ€å°å€¼åœ¨ x = 2 å¤„ï¼Œf(2) = -1")
    
    # æµ‹è¯•ä¸åŒå­¦ä¹ ç‡
    learning_rates = [0.01, 0.1, 0.5]
    starting_point = 0.0
    
    for lr in learning_rates:
        print(f"\n--- å­¦ä¹ ç‡: {lr} ---")
        optimizer = GradientDescent(learning_rate=lr)
        final_x = optimizer.optimize(starting_point)
        final_fx = quadratic_function(final_x)
        
        print(f"æœ€ç»ˆä½ç½®: x = {final_x:.4f}")
        print(f"æœ€ç»ˆå‡½æ•°å€¼: f(x) = {final_fx:.4f}")
        print(f"è¿­ä»£æ¬¡æ•°: {len(optimizer.history)}")
        
        # æ˜¾ç¤ºå‰å‡ æ¬¡è¿­ä»£
        print("å‰5æ¬¡è¿­ä»£:")
        print("è¿­ä»£ |    x    |  f(x)  | æ¢¯åº¦  ")
        print("----|---------|--------|-------")
        for i in range(min(5, len(optimizer.history))):
            x, fx, grad = optimizer.history[i]
            print(f" {i:2d} | {x:7.3f} | {fx:6.3f} | {grad:5.2f}")
    
    print(f"\nåˆ†æä¸åŒå­¦ä¹ ç‡çš„å½±å“ï¼š")
    print("â€¢ å­¦ä¹ ç‡å¤ªå°ï¼šæ”¶æ•›æ…¢ï¼Œéœ€è¦æ›´å¤šè¿­ä»£")
    print("â€¢ å­¦ä¹ ç‡é€‚ä¸­ï¼šå¿«é€Ÿæ”¶æ•›åˆ°æœ€ä¼˜è§£")
    print("â€¢ å­¦ä¹ ç‡å¤ªå¤§ï¼šå¯èƒ½éœ‡è¡æˆ–å‘æ•£")
    
    # æ¼”ç¤ºä¸åŒèµ·å§‹ç‚¹
    print(f"\næµ‹è¯•ä¸åŒèµ·å§‹ç‚¹ï¼ˆå­¦ä¹ ç‡=0.1ï¼‰ï¼š")
    start_points = [-2, 0, 5, 10]
    
    for start_x in start_points:
        optimizer = GradientDescent(learning_rate=0.1)
        final_x = optimizer.optimize(start_x)
        final_fx = quadratic_function(final_x)
        
        print(f"èµ·å§‹ç‚¹ {start_x:2d} -> æœ€ç»ˆç‚¹ {final_x:.3f} (f={final_fx:.3f}) ç”¨äº† {len(optimizer.history)} æ¬¡è¿­ä»£")
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ æ¢¯åº¦ä¸‹é™æ²¿è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨")
    print("â€¢ å­¦ä¹ ç‡æ§åˆ¶æ­¥é•¿å¤§å°")
    print("â€¢ å‡¸å‡½æ•°èƒ½ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜")
    print("â€¢ æ·±åº¦å­¦ä¹ ä¸­æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç½‘ç»œæƒé‡")

def exercise_6_backpropagation():
    """ç»ƒä¹ 6ï¼šæ‰‹å·¥è®¡ç®—åå‘ä¼ æ’­"""
    print("\n" + "="*50)
    print("ç»ƒä¹  6: åå‘ä¼ æ’­ç®—æ³•æ‰‹å·¥è®¡ç®—")
    print("="*50)
    
    print("ç½‘ç»œç»“æ„ï¼šè¾“å…¥å±‚(2) -> éšè—å±‚(2) -> è¾“å‡ºå±‚(1)")
    print("æ¿€æ´»å‡½æ•°ï¼šSigmoid")
    print("æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®")
    
    def sigmoid(x):
        return 1.0 / (1.0 + math.exp(-x))
    
    def sigmoid_derivative(x):
        s = sigmoid(x)
        return s * (1 - s)
    
    # ç½‘ç»œå‚æ•°ï¼ˆå›ºå®šå€¼ç”¨äºæ¼”ç¤ºï¼‰
    print(f"\nç½‘ç»œå‚æ•°ï¼š")
    
    # éšè—å±‚æƒé‡å’Œåç½®
    W1 = [[0.5, 0.3], [0.2, 0.8]]  # 2x2
    b1 = [0.1, 0.4]
    
    # è¾“å‡ºå±‚æƒé‡å’Œåç½®  
    W2 = [0.6, 0.7]  # 1x2
    b2 = 0.2
    
    print(f"éšè—å±‚æƒé‡ W1:")
    for i, row in enumerate(W1):
        print(f"  ç¥ç»å…ƒ{i+1}: {row}")
    print(f"éšè—å±‚åç½® b1: {b1}")
    print(f"è¾“å‡ºå±‚æƒé‡ W2: {W2}")
    print(f"è¾“å‡ºå±‚åç½® b2: {b2}")
    
    # è¾“å…¥å’Œç›®æ ‡
    inputs = [0.8, 0.2]
    target = 0.9
    
    print(f"\nè¾“å…¥: {inputs}")
    print(f"ç›®æ ‡è¾“å‡º: {target}")
    
    print(f"\n=== å‰å‘ä¼ æ’­ ===")
    
    # éšè—å±‚è®¡ç®—
    print(f"éšè—å±‚è®¡ç®—:")
    z1 = []
    a1 = []
    for i in range(2):
        z = sum(W1[i][j] * inputs[j] for j in range(2)) + b1[i]
        z1.append(z)
        a = sigmoid(z)
        a1.append(a)
        print(f"  ç¥ç»å…ƒ{i+1}: z={z:.4f}, a=sigmoid({z:.4f})={a:.4f}")
    
    # è¾“å‡ºå±‚è®¡ç®—
    print(f"è¾“å‡ºå±‚è®¡ç®—:")
    z2 = sum(W2[i] * a1[i] for i in range(2)) + b2
    a2 = sigmoid(z2)
    print(f"  z={z2:.4f}, output=sigmoid({z2:.4f})={a2:.4f}")
    
    # æŸå¤±è®¡ç®—
    loss = 0.5 * (a2 - target)**2
    print(f"\næŸå¤±: L = 0.5*(é¢„æµ‹-ç›®æ ‡)Â² = 0.5*({a2:.4f}-{target})Â² = {loss:.4f}")
    
    print(f"\n=== åå‘ä¼ æ’­ ===")
    
    # è¾“å‡ºå±‚è¯¯å·®
    print(f"è¾“å‡ºå±‚è¯¯å·®:")
    delta2 = (a2 - target) * sigmoid_derivative(z2)
    print(f"  Î´â‚‚ = (aâ‚‚-t) * Ïƒ'(zâ‚‚) = ({a2:.4f}-{target}) * {sigmoid_derivative(z2):.4f} = {delta2:.4f}")
    
    # éšè—å±‚è¯¯å·®
    print(f"éšè—å±‚è¯¯å·®:")
    delta1 = []
    for i in range(2):
        d = delta2 * W2[i] * sigmoid_derivative(z1[i])
        delta1.append(d)
        print(f"  Î´â‚[{i+1}] = Î´â‚‚ * Wâ‚‚[{i+1}] * Ïƒ'(zâ‚[{i+1}]) = {delta2:.4f} * {W2[i]} * {sigmoid_derivative(z1[i]):.4f} = {d:.4f}")
    
    # æƒé‡æ¢¯åº¦
    print(f"\næƒé‡æ¢¯åº¦:")
    learning_rate = 0.5
    
    # è¾“å‡ºå±‚æƒé‡æ¢¯åº¦
    print(f"è¾“å‡ºå±‚æƒé‡æ¢¯åº¦:")
    for i in range(2):
        grad = delta2 * a1[i]
        new_weight = W2[i] - learning_rate * grad
        print(f"  âˆ‚L/âˆ‚Wâ‚‚[{i+1}] = Î´â‚‚ * aâ‚[{i+1}] = {delta2:.4f} * {a1[i]:.4f} = {grad:.4f}")
        print(f"  Wâ‚‚[{i+1}] = {W2[i]} - {learning_rate}*{grad:.4f} = {new_weight:.4f}")
    
    # éšè—å±‚æƒé‡æ¢¯åº¦
    print(f"éšè—å±‚æƒé‡æ¢¯åº¦:")
    for i in range(2):
        for j in range(2):
            grad = delta1[i] * inputs[j]
            new_weight = W1[i][j] - learning_rate * grad
            print(f"  âˆ‚L/âˆ‚Wâ‚[{i+1}][{j+1}] = Î´â‚[{i+1}] * x[{j+1}] = {delta1[i]:.4f} * {inputs[j]} = {grad:.4f}")
            print(f"  Wâ‚[{i+1}][{j+1}] = {W1[i][j]} - {learning_rate}*{grad:.4f} = {new_weight:.4f}")
    
    print(f"\nç»ƒä¹ æ€»ç»“ï¼š")
    print("â€¢ åå‘ä¼ æ’­ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦")
    print("â€¢ è¯¯å·®ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚ä¼ æ’­")
    print("â€¢ æ¯å±‚çš„æ¢¯åº¦ä¾èµ–äºä¸‹ä¸€å±‚çš„è¯¯å·®")
    print("â€¢ æƒé‡æ›´æ–°ä½¿ç”¨æ¢¯åº¦ä¸‹é™è§„åˆ™")

def create_practice_roadmap():
    """åˆ›å»ºæ·±åº¦å­¦ä¹ å®è·µè·¯çº¿å›¾"""
    print("\n" + "="*50)
    print("æ·±åº¦å­¦ä¹ å®è·µè·¯çº¿å›¾")
    print("="*50)
    
    roadmap = {
        "åˆçº§é˜¶æ®µ": [
            ("æ„ŸçŸ¥æœºå®ç°", "ç†è§£åŸºæœ¬çš„çº¿æ€§åˆ†ç±»å™¨"),
            ("å¤šå±‚æ„ŸçŸ¥æœº", "æŒæ¡åå‘ä¼ æ’­ç®—æ³•"),
            ("æ¿€æ´»å‡½æ•°å¯¹æ¯”", "ç†è§£éçº¿æ€§å˜æ¢çš„ä½œç”¨"),
            ("æŸå¤±å‡½æ•°å®éªŒ", "æŒæ¡ä¸åŒä»»åŠ¡çš„æŸå¤±é€‰æ‹©"),
            ("æ¢¯åº¦ä¸‹é™è°ƒä¼˜", "ç†è§£ä¼˜åŒ–ç®—æ³•åŸç†")
        ],
        
        "ä¸­çº§é˜¶æ®µ": [
            ("å·ç§¯ç¥ç»ç½‘ç»œ", "å®ç°åŸºæœ¬çš„å›¾åƒåˆ†ç±»"),
            ("å¾ªç¯ç¥ç»ç½‘ç»œ", "å¤„ç†åºåˆ—æ•°æ®"),
            ("LSTM/GRU", "è§£å†³é•¿åºåˆ—é—®é¢˜"),
            ("æ‰¹å½’ä¸€åŒ–", "åŠ é€Ÿè®­ç»ƒå’Œæé«˜ç¨³å®šæ€§"),
            ("Dropoutæ­£åˆ™åŒ–", "é˜²æ­¢è¿‡æ‹Ÿåˆ")
        ],
        
        "é«˜çº§é˜¶æ®µ": [
            ("æ³¨æ„åŠ›æœºåˆ¶", "ç†è§£æ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹"),
            ("Transformer", "æŒæ¡ç°ä»£NLPæ¶æ„"),
            ("ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ", "å­¦ä¹ ç”Ÿæˆæ¨¡å‹"),
            ("å˜åˆ†è‡ªç¼–ç å™¨", "ç†è§£æ½œåœ¨ç©ºé—´å»ºæ¨¡"),
            ("å¼ºåŒ–å­¦ä¹ åŸºç¡€", "æ¢ç´¢æ™ºèƒ½ä½“å­¦ä¹ ")
        ],
        
        "é¡¹ç›®å®æˆ˜": [
            ("å›¾åƒåˆ†ç±»é¡¹ç›®", "å®Œæ•´çš„CNNé¡¹ç›®æµç¨‹"),
            ("æ–‡æœ¬åˆ†ç±»é¡¹ç›®", "NLPå®è·µåº”ç”¨"),
            ("æ—¶é—´åºåˆ—é¢„æµ‹", "RNNåœ¨å®é™…æ•°æ®ä¸Šçš„åº”ç”¨"),
            ("æ¨èç³»ç»Ÿ", "æ·±åº¦å­¦ä¹ åœ¨æ¨èä¸­çš„åº”ç”¨"),
            ("ç”Ÿæˆæ¨¡å‹é¡¹ç›®", "åˆ›å»ºè‰ºæœ¯ä½œå“æˆ–æ–‡æœ¬")
        ]
    }
    
    for stage, exercises in roadmap.items():
        print(f"\nã€{stage}ã€‘")
        for i, (name, desc) in enumerate(exercises, 1):
            print(f"{i}. {name}")
            print(f"   ç›®æ ‡: {desc}")
    
    print(f"\nå­¦ä¹ å»ºè®®:")
    print("â€¢ å¾ªåºæ¸è¿›ï¼Œä¸è¦è·³è·ƒå¼å­¦ä¹ ")
    print("â€¢ æ¯ä¸ªç»ƒä¹ éƒ½è¦åŠ¨æ‰‹å®ç°")
    print("â€¢ ç†è§£åŸç†æ¯”è®°ä½ä»£ç æ›´é‡è¦")
    print("â€¢ å¤šåšå®éªŒï¼Œè§‚å¯Ÿå‚æ•°å˜åŒ–çš„å½±å“")
    print("â€¢ ç»“åˆç†è®ºå­¦ä¹ å’Œå®è·µç»ƒä¹ ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æ·±åº¦å­¦ä¹ å®è·µç»ƒä¹ ")
    print("=" * 60)
    
    exercises_introduction()
    exercise_1_perceptron()
    exercise_2_mlp_xor() 
    exercise_3_mini_cnn()
    exercise_4_simple_rnn()
    exercise_5_gradient_descent()
    exercise_6_backpropagation()
    create_practice_roadmap()
    
    print("\n" + "=" * 60)
    print("ğŸ“ ç»ƒä¹ æ€»ç»“")
    print()
    print("é€šè¿‡è¿™äº›ç»ƒä¹ ä½ å­¦åˆ°äº†:")
    print("â€¢ æ„ŸçŸ¥æœºçš„çº¿æ€§åˆ†ç±»èƒ½åŠ›å’Œå±€é™æ€§")
    print("â€¢ å¤šå±‚ç½‘ç»œè§£å†³éçº¿æ€§é—®é¢˜çš„åŸç†")
    print("â€¢ å·ç§¯æ“ä½œæå–å›¾åƒç‰¹å¾çš„è¿‡ç¨‹")
    print("â€¢ RNNå¤„ç†åºåˆ—æ•°æ®çš„è®°å¿†æœºåˆ¶")
    print("â€¢ æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•çš„å·¥ä½œåŸç†")
    print("â€¢ åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦çš„è¯¦ç»†æ­¥éª¤")
    print()
    print("ç»§ç»­å­¦ä¹ å»ºè®®:")
    print("â€¢ å®ç°æ›´å¤æ‚çš„ç½‘ç»œæ¶æ„")
    print("â€¢ åœ¨çœŸå®æ•°æ®é›†ä¸ŠéªŒè¯ç®—æ³•")
    print("â€¢ å­¦ä¹ ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶")
    print("â€¢ å…³æ³¨æœ€æ–°çš„ç ”ç©¶è¿›å±•")
    print("â€¢ å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®ä»£ç ")
    print()
    print("è®°ä½ï¼šç†è§£åŸç†æ¯”è®°ä½ä»£ç æ›´é‡è¦ï¼")

if __name__ == "__main__":
    main()